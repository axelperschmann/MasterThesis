\chapter{Orderbook Agents}
This chapter describes several \emph{Orderbook Agents}, implemented to learn optimal \emph{Submit \& Revise} trading strategies from a given training data set.


\section{Implementation}
\label{chap:experiments:implementation}
Each \emph{Orderbook Agents} inherits some commonly shared functionality from the super class \lstinline!class RL_Agent_Base!. This base class provides a consistent framework, allowing to swap the logic behind \lstinline!learn_from_samples! and \lstinline!predict! only, while general functionality like \lstinline!load!, \lstinline!save!, \lstinline!plot_heatmap_Q! and \lstinline!collect_samples! may be reused.\\

An \emph{Orderbook Agent} holds the complete list of environment parameters, used to initialize the \ac{OTS}. As such, different definitions for the optimal trading problem require different Orderbook Agents.


\subsection{Backward Sampling \& Learning}
The process of sampling from training data and learning optimal state-action values has been split into two independent phases, mostly due to performance\footnote{The backward sampling phase for the example mentioned in \Cref{chap:backwardlearning}, took roughly 10 hours, even though the work load was distributed among 24 CPU's.} reasons. In the \emph{sampling phase}, all possible combinations of actions and private variables are evaluated and stored as 5-tuples $sample=(state, action, cost, timestamp, new\_state)$. As market variables are supposedly not influenced by the agents behavior (see \Cref{chap:statespace}), it suffices to remember the orderbooks time point, to allow adjacent substitution of market variables.\\

Samples are stored as DataFrame and may be exported and reused by other agents\footnote{Both agents should refer to the same environment settings, as samples collected from different environments (\eg trading horizon of $H=8min$ vs. $H=60min$) may not be compatible.}, as wished. This allows to skip the time consuming process of backward sampling. Before the action learning phase starts, the collected samples may be enhanced by a variety of market variables, discretized or untouched. The helper function \lstinline!addMarketFeatures_toSamples()! retrospectively adds market variables to the agents samples DataFrame.\\

If desired, features are discretized according to their individual value range.\\
\Eg a chosen resolution of 3, evenly transforms features $o_m$ into $o_{m\_disc3}$, according to automatically determined boundaries: \\
0 (if $o_m< 1/3$ quantile), 1 (if $1/3 \leq o_m < 2/3$ quantile) and 2 (if $o_m \geq 2/3$ quantile). Quantiles arise from the training data and are applied to test data unaltered.\\

The learning phase starts hereinafter, whereas three types of orderbook agents have been implemented and experimented with, each of which employs different techniques:

\begin{description}
\item[QTable\_Agent] : Dynamic Programming as described in \Cref{chap:backwardlearning}.\\
Expected state-values are stored in a simple lookup table, hence discretization of the state space is required. For each state, a vector of length $L=num\_actions$ is maintained, of which the first argmin refers to the optimal action. The \emph{QTable} does not generalize to previously unobserved states and returns the very first action (here -4) in such a case. For proper cost-updates an additional \emph{NTable} is maintained, referencing the particular number of observations made.\\
QTable and NTable entries are computed in an iterative manner. In the first round, all samples with \lstinline!time_left=1! are evaluated, in the second round all samples with \lstinline!time_left=2!, \etc The private variable \lstinline!volume! is discretized according to the specified resolution and linear cost scaling (see \Cref{chap:backwardalgorithm:discussion:costscaling}) is performed.

\item[BatchTree\_Agent]: Tree-Based Batch Mode Reinforcement Learning \Cite{Ernst:2005:TreeBasedBatchModeRL}.\\
An ensemble of 100 decision trees, with a maximum allowed depth of 20 is fed with the full batch of samples simultaneously, no discretization required. $T*2$ learning rounds are performed, to completely allow expected costs to unfold over the given time horizon.\\
The state space is enlarged by an extra dimension for the chosen action. Predicting the optimal action to choose for a given state, the ensemble must be queried $L=num\_actions$ times, from which the argmin refers to the optimal action.

\item[NN\_Agent] : Neural fitted Q Iteration \Cite{Riedmiller:2005:NFQ}.\\
A simple multi layer perceptron with 400 hidden units and 1 output neuron is trained in mini batches of size 4.096, optimized over multiple epochs by Adam\Cite{Kingma:2014:Adam} with a learning rate of 0.01. As with the BatchTree\_Agent, no state space discretization is required and actions are included into the state space. $L$ predictions are required, to obtain the optimal action.
\end{description}



\subsection{Forward Sampling \& Learning}
In order to create more realistic orderbook shapes (see \Cref{chap:backwardalgorithm:discussion:markovianassumption}), an forward sampling method is presented.


\begin{lstlisting}[frame=single, breaklines=true, basicstyle=\scriptsize, caption=Forward sampling approach., label=lst:forward:pseudocode]
collect_samples_forward(V, H, T, I, L, E)
    While(not end of data)
        init OTS with V=100%
            For epoch=0 to E
                reset OTS at random startpoint
	            Transform (orderbook) -> o_1 ... o_R
	            Apply e-greedy strategy until V=0%
		    Store samples in DataFrame
        occasionally retrain strategy with new collected samples (+ experience replay)
		    
\end{lstlisting}







\section{Backward Sampling Experiments}
\label{chap:experiments}
Various experiments have been conducted to examine the agents ability to find optimal solutions to the problem of optimized trade execution. The recorded orderbook snapshots for currency pair USDT/BTC (see \Cref{chap:dataorigin}) have been split into training period (Nov, 10th 2016 - Apr, 30th 2017) and test period (May 2017).\\

The studied agents refer to the very same environment settings. Their common task is to buy Bitcoins worth of 70.000\$ within a trading horizon of 60 minutes. As such the training set translates into 4.154 orderbook windows, while the test set gives 724 orderbook windows. If not stated otherwise, a period\_length of 15 minutes is assumed, such that the \ac{OTS} expects up to 4 order limit prices. After an initial backward sampling phase, the obtained samples DataFrame makes the base for varying learning phases.

\subsection{Action-Limit Mapping}
\label{chap:exp:actionlimitmapping}
As mentioned in \Cref{chap:backwardalgorithm:discussion:actionspace}, it seems pointless to force actions to cross the bid-ask-spread before any orders can be matched. \Cref{fig:actionlimitmapping} shows exemplary for November 2016, how the choice of the limit base affects the agents trading performance. While buy orders forcing agents to cross the bid-ask-spread did indeed benefit from the two market variables \lstinline!spread! and \lstinline!marketSpread!, this was not necessarily the case for agents which had the limit base fixed to the other best price. As the latter agents consistently showed better performance, choice has been made to avoid crossing the spread henceforth.

\begin{figure}[ht]
	\centering
	
	\begin{subfigure}[b]{0.6\textwidth}
        		\centering
        		\includegraphics[width=\textwidth]{content/drawings/performance_limitBase_nov}
        		\caption{Slippage.}
		\label{fig:actionlimitmapping:plot}
    	\end{subfigure}%
	\begin{subfigure}[b]{0.40\textwidth}
        		\centering
		\scalebox{0.7}{
\begin{tabular}{lrr}
\toprule
{} &           slippage & performance \\
\midrule
currBid         	     &  220.19 & 92.6\%\\
currBid\_mSpread &  218.71 & 91.9\%\\
currBid\_spread     &  215.76 & 90.7\%\\
\midrule
currAsk                  &  214.00 & 90.0\%\\
currAsk\_mSpread &  214.53 &90.2\%\\
currAsk\_spread    &  215.38 & 90.6\%\\
\midrule
S\&L: 5         	      & 237.75 & 100.0\%\\
MarketOrder          & 737.58 & 310.2\%\\
\bottomrule
\end{tabular}	
		}  \vspace{1.5cm}      		 
        		\caption{Mean slippage.}
		\label{fig:actionlimitmapping:mean}
    	\end{subfigure}

	\caption{Evaluating the impact of different limit base levels.}
	\label{fig:actionlimitmapping}
\end{figure}

In order to produce comparable results, all subsequent experiments are based on the same set of 15 actions: $[-4, -3, ..., 8, 9, 10]$. In line with the formulas presented in \Cref{chap:backwardalgorithm:discussion:actionspace}, these actions translate into order limits deviating by -0.4\% to +1.0\% from the current best price.



\subsection{Baseline}
\label{chap:experiments:baseline}
Simple \ac{SL} strategies (see \Cref{chap:tradingstrategies}) and immediate market order placements serve as benchmark for the performance measurement.\\

Due to bursting Bitcoin prices (see \Cref{fig:ploniexPriceHistory}), the investigated sum of 70.000\$ constitutes a declining contingent of the total market volume. \Cref{fig:runningmean:marketPrice} shows a running average over the amount of slippage, as induced by immediate market orders. Concurrent to declining slippage, the optimal \ac{SL} actions become less aggressive as time passes. The green, dashed lines show the respective average over the train period, which significantly differs from the red, dotted line referring to the average over the test period. \Cref{fig:bestAction} shows the average costs, induced by varying \ac{SL} strategies within the training period.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
        		\centering
        		\includegraphics[width=\textwidth]{content/drawings/runningMean240_MarketPrice}
        		\caption{Observed Market Order Slippage.}
		\label{fig:runningmean:marketPrice}
    	\end{subfigure}%
	\begin{subfigure}[b]{0.5\textwidth}
        		\centering
        		\includegraphics[width=\textwidth]{content/drawings/runningMean240_bestAction}
        		\caption{Best S\&L action.}
		\label{fig:runningmean:bestaction}
    	\end{subfigure}

	\caption{Concurrent to declining slippage, the optimal \ac{SL} actions become less aggressive as time passes.}
	\label{fig:runningmean}
\end{figure}


\begin{figure}[ht]
        	\centering
        	\includegraphics[width=\textwidth]{content/drawings/bestActionTrain}
	\caption{Average costs induced by the varying \ac{SL} actions over the full training period. In average, action 4 performed best.}
	\label{fig:bestAction}
\end{figure}

In order to provide a more realistic and unexaggerated baseline, the optimal \ac{SL} action is estimated from the test period. As such, performance of subsequents experiments are compared to the performance of a simple market order and the \ac{SL} strategy "2", with the initial order limit fixed to 1.002 times the initial ask (\ie $+= 0.2\%$).




\subsection{Additional Market Variables}
\label{chap:exp:additionalmarketvars}
In addition to the market variables proposed in \Cref{chap:actionspace}, the following market level 2 features have been examined: \lstinline!marketPrice_*! describes the relative difference between current center\_price and the worst price that must be paid in case of simple market orders. \lstinline!marketPrice_buy_worst! \Cref{tab:eval:additionalMarketVariables} shows the performance of individual QTable agents trained on private variables plus one discretized market variable attached at a time. While observed trading costs undercut costs induced by simple market orders by almost -47.13\%, the gain in comparison to simple \ac{SL} strategies is far smaller: -7.43\%. In contrast to the results reported by Nevmyvaka \etal \Cite{Nevmyvaka:2006}, the market variable \lstinline!spread! yields only an improvement of -2.14\% (compared to -7.97\%) over the plain \lstinline!VolTime!-agent.\\


\begin{table}[ht]
	\centering
	
		\scalebox{0.6}{
		\begin{tabular}{lrr|rrrr}
\toprule
{} &  slippage &     std &    perf\_2 &  perf\_4 &  perf\_M &  perf\_VolTime \\
\midrule
center\_orig\_disc3            	&    149.57 &  420.14 &   94.37\% &   84.81\% &   53.90\% &   99.30\% \\
ImmCost\_buy\_worst\_disc3  	&    148.35 &  361.17 &   93.61\% &   84.11\% &   53.46\% &   98.49\% \\
ImmCost\_sell\_worst\_disc3 	&    \color{red}146.72 &  385.13 &   \color{red}92.57\% &   \color{red}83.19\% &   \color{red}52.87\% &   \color{red}97.41\% \\
ImmCost\_spread\_disc3     		&    150.27 &  355.46 &   94.82\% &   85.20\% &   54.16\% &   99.77\% \\
ImmCost\_imbalance\_disc3     	&    148.42 &  358.49 &   93.64\% &   84.15\% &   53.49\% &   98.53\% \\
sharecount\_buy\_disc3         	&    149.57 &  420.14 &   94.37\% &   84.81\% &   53.90\% &   99.30\% \\
sharecount\_imbalance\_disc3  	&    148.57 &  353.91 &   93.74\% &   84.24\% &   53.54\% &   98.64\% \\
sharecount\_sell\_disc3        	&    149.57 &  420.14 &   94.37\% &   84.81\% &   53.90\% &   99.30\% \\
sharecount\_spread\_disc3      	&    147.57 &  \color{red}350.33 &   93.11\% &   83.67\% &   53.18\% &   97.97\% \\
spread\_disc3                 	&    147.41 &  370.96 &   93.01\% &   83.58\% &   53.12\% &   97.86\% \\
\midrule
ob\_direction\_disc3           	&     65.80 &  335.76 &   41.52\% &   37.31\% &   23.71\% &   43.68\% \\
future\_center5\_disc3         	&    133.58 &  377.88 &   84.29\% &   75.74\% &   48.14\% &   88.69\% \\
future\_center15\_disc3        	&    107.74 &  364.92 &   67.98\% &   61.09\% &   38.83\% &   71.53\% \\
future\_center60\_disc3        	&    139.20 &  398.05 &   87.83\% &   78.93\% &   50.17\% &   92.42\% \\
\midrule
VolTime                      	&    150.63 &  358.66 &   95.04\% &   85.40\% &   54.28\% &  100.00\% \\
\midrule
2                            	&    158.49 &  400.59 &  100.00\% &   89.86\% &   57.12\% &  105.22\% \\
4                            	&    176.37 &  273.58 &  111.28\% &  100.00\% &   63.56\% &  117.09\% \\
MarketOrder                  	&    277.49 &  158.66 &  175.08\% &  157.33\% &  100.00\% &  184.22\% \\
\bottomrule
\end{tabular}}           		 
        		\caption{Average trading costs, as observed within the test period, show the effect of adding individual, discretized market variables to the state space of QTable agents.}
		\label{tab:eval:additionalMarketVariables}

\end{table}

The exact reason, why individual market variables have such little effect, is unclear, but one possible explanations lies in the data set employed. The original experiment assessed a different data quality. Due to the low resolution of the available orderbook snapshots, a large fraction of the market activity is inaccessible and consequently the majority of trading opportunities are missed by the agents. Furthermore, the minute time-scaled limit order book data inevitably requires the trading horizon to be rather long. Experiments with shorter time horizons nullified the achievable savings, while longer time horizons led to unacceptable computation times. Consequently, the agents where trained on 4.109 sixty-minute orderbook windows, while Nevmyvaka \etal invoked 45.000 two-minute orderbook windows.\\

In order to proof the algorithms general ability to find costs reducing order limits, look-ahead features\footnote{look-ahead features provide a glance into the future, and are thus equivalent to cheating.} were added to the universe of market variables. The hypothetical knowledge about future price trends (\ie percentual changes between the current center price and the center price in 5, 15 and 60 minutes respectively) reduced observed trading costs by -32.02\%.\\

The largest impact (-58.48\%) was observed for the look-ahead feature \lstinline!ob_direction!, which marks the general price trend of the currently observed orderbook window. In contrast to the \lstinline!future_enter*! variables, it's value stays constant within individual orderbook windows: \lstinline!orderbook[-1].get_center() / orderbook[0].get_center()!, which seems to provoke more stable strategies.\\

\subsection*{Constant Market Variables}
The findings from the look-ahead features encourage a tailing experiment. Rather than observing market variables at the actual time, always the market situation, observed at \lstinline!t=0! is selected.



\subsection{Cost function}
default: slippage based on initial\_center

experimental: Improvement over MarketPrice



\subsection*{Discretization Resoution}
\label{chap:experiments:discretization}
Comparison of disc3, disc5, disc9

\subsection*{Function Approximation}
\begin{itemize}
\item RandomForest (BatchTree Agent)
\item NN Agent
\end{itemize}



\subsection{Forward Sampling}
\begin{itemize}
\item Growing batch learning
\item Experience Replay (NN\_Agent)
\item Exploration vs avoidance of repeatedly trying same actions.
\item Markov Property violated
\item Realistic samples, no rounding. Better fit for \emph{curious} masterbook shapes as described in \Cref{chap:modelcorrectness}?!
\end{itemize}














\cleardoublepage{}