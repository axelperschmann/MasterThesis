\chapter{Conclusion}\label{chap:conclusion}
The goal of this thesis was to reduce trade execution costs by applying reinforcement learning on the problem of optimized trade execution. While higher level trading strategies analyze a universe of assets and decide on how many shares of individual assets shall be bought (or sold), optimized trading execution tackles the consecutive phase. Here, the trading decision itself is not questioned, but simply executed at the best possible rate.\\

As reinforcement learning requires agents to interact with an environment, a trading simulation framework was implemented. The accrued \ac{OTS} provides a full-featured reinforcement learning environment, that simulates the execution of orders on historic data and returns vital feedback to steer the agent towards optimal decisions. Additionally, it serves as a backtesting framework in order to evaluate the cost reducing capabilities of different trading strategies.\\

This thesis assessed a data set of self recorded bitcoin \ac{LOB} snapshots on a rather low minute-scale basis, which stands in contrast to most other works, which had the advantage of large, proprietary data sets on millisecond basis from traditional stock exchanges. The discrepancies between data quality and observed price volatility make it difficult to compare achieved performances to other studies. Very dense \ac{LOB}s and large price fluctuations in the bitcoin data set suggest, that large parts of the observed slippage stem from price fluctuations, which are harder to predict than slippage that originates from eating into the respective \ac{LOB}s.\\

An existing reinforcement learning approach, deriving valuable strategies from a discretized state space in a brute force manner, has been replicated and transferred to the data set at hand. If trained on private variables only, the replicated algorithm could achieve an improvement of $-2.76\%$ over the more simple \ac{SL} strategies. If compared to the \ac{SL} strategy that performed best on the training set (rather than on the test set: see \Cref{chap:experiments:baseline}), this value increases to $-14.60\%$.\\

In the scope of this thesis, the exploration phase of the backward learning agent has been overhauled, such that it now accounts for self-induced impact on the market situation. By incorporating preceding trades properly, the agents cost reducing capabilities could be raised from $-2.76\%$ to $-4.55\%$. As in the original approach, adding (one) additional market variable(s) further increased this performance: Knowledge about the current spread led to performance gains of $-4.84\%$ for the replicated and $-9.70\%$ for the overhauled algorithm.\\

Finally, a novel reinforcement learning approach was presented, that applies growing batch reinforcement learning on the problem of optimal trade execution. The proposed forward-sampling algorithm samples from a more realistic, continuous state space and outperforms the aforementioned algorithms with an improvement of $-11.41\%$ over the more simple \ac{SL} strategy. Similar to above, this translates into an average improvement of $-22.19\%$ over the \ac{SL} strategy that performed best on the training set.\\

In absolute values, the slippage for a total order volume of $70.000\$$ could be halved from $277\$$ (as achieved by simple market orders) to $\sim137\$$ ($-50.55\%$). As high level strategies typically instruct many such (or even larger) trades in parallel, this scales to considerable amounts of saved money.





\section{Future Work}
The most promising start point for future work lies in further improvements to the novel forward learning algorithm. The utilization of (deep) neural networks and replay memory may help to find better generalizations of the state-action functions. The generalizing of Q-learning to work with a continuous action space potentially allows for more appropriate actions. 

As the experiments with look-ahead features revealed, a great potential may be found in specialized time series prediction methods as \eg Prevedex STaR \Cite{STAR}. An improved ability to (partially) predict future price trends possibly helps to deal with highly volatile markets.

