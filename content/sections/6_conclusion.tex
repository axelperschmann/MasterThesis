\chapter{Conclusion}\label{chap:conclusion}
The goal of this thesis was to reduce trade execution costs by applying reinforcement learning on the problem of optimized trade execution. While higher level trading strategies analyze a universe of assets and decide on how many shares of individual assets shall be bought (or sold), optimized trading execution tackles the consecutive phase. Here, the trading decision itself is not questioned, but executed at the best possible rate.\\

As reinforcement learning requires agents to interact with an environment, a trading simulation framework was implemented. The accrued \ac{OTS} provides a full-featured reinforcement learning environment, that simulates the execution of orders on historic data and returns vital feedback to steer the agent towards optimal decisions. Additionally, it serves as a backtesting framework in order to evaluate the cost reducing capabilities of different trading strategies.\\

This thesis assessed a data set of self recorded bitcoin \ac{LOB} snapshots on a rather low minute-scale basis, which stands in contrast to most other works, which had the advantage of large, proprietary data sets on millisecond basis from traditional stock exchanges. The discrepancies between data quality and observed price volatility make it difficult to compare achieved performances to other studies. Very dense \ac{LOB}s and large price fluctuations in the bitcoin data set suggest, that large parts of the observed slippage stem from price fluctuations, which are harder to predict than slippage that originates from eating into the respective \ac{LOB}s.\\

An existing reinforcement learning approach, deriving valuable strategies from a discretized state space in a brute force manner, has been replicated and transferred to the data set at hand. While its authors claimed improvements of $-27.16\%$ to $-35.50\%$ over more simple \ac{SL} strategies, if trained on private variables only, the replicated algorithm could only achieve an improvement of $-2.76\%$ on the bitcoin data set. If compared to the \ac{SL} strategy that performed best on the training set (rather than on the test set: see \Cref{chap:experiments:baseline}), this value increases to $-14.60\%$, which is still only half of what was claimed on traditional stock data. This observation gives reason to hope that the overhauled approaches perform better on other data sets.\\

In the scope of this thesis, the exploration phase of the backward learning agent has been overhauled, such that it now accounts for self-induced impact on the market situation. By incorporating preceding trades properly, the agents cost reducing capabilities could be raised from $-2.76\%$ to $-4.55\%$. As in the original approach, adding (one) additional market variable(s) further increased this performance: Knowledge about the current spread led to performance gains of $-4.84\%$ for the replicated and $-9.70\%$ for the overhauled algorithm.

Finally, a novel reinforecement learning approach was presented, that applies growing batch reinforcement learning on the problem of optimal trade execution. The proposed forward-sampling method samples from a more realistic, continuous state space and outperforms the formentioned algorithms with an improvement of $-11.41\%$ over the more simple \ac{SL} strategy.\\

In absolute values, the slippage for a total order volume of $70.000\$$ could be halved from $277\$$ (as achieved by simple market orders) to $\sim137\$$. This does not sound like a great benefit, but as high level strategies may instruct many such trades in parallel, this scales to considerable amounts of saved money. Likewise the potential savings grow exponentially with the size of instructed order.





\section{Future Work}
First of all, it would be interesting to evaluate the current state of the proposed forward-learning approach on traditional high-frequency stock data, in order to judge its effective performance in a descriptive manner.\\

However, the most promising start point for future work lies in further improvements to the novel forward learning algorithm. The utilization of (deep) neural networks and replay memory may help to find better generalizations of the state-action functions. The generalizing of Q-learning to work with a continuous action space potentially allows for more appropriate actions. As the experiments with look-ahead features revealed, a great potential may be found in specialized time series prediction methods as \eg Prevedex STaR \Cite{STAR}. An improved ability to (partially) predict future price trends possibly helps to deal with highly volatile markets.

