\chapter{Conclusion}\label{chap:conclusion}
The goal of this thesis was to reduce trade execution costs by applying reinforcement learning on the problem of optimized trade execution. While higher level trading strategies analyze a universe of assets and decide on how many shares of individual assets shall be bought (or sold), optimized trading execution tackles the consecutive phase. Here, the trading decision itself is not questioned, but executed at the best possible rate.\\

As reinforcement learning requires agents to interact with an environment, a trading simulation framework was implemented. The accrued \ac{OTS} provides a full-featured reinforcement learning environment, that simulates the execution of orders on historic data and returns vital feedback to steer the agent towards optimal decisions. Additionally, it serves as a backtesting framework in order to evaluate the cost reducing capabilities of different trading strategies.\\

This thesis assessed a data set of self recorded bitcoin \ac{LOB} snapshots on a rather low minute-scale basis, which stands in contrast to most other works, which had the advantage of large, proprietary data sets on millisecond basis from traditional stock exchanges. The discrepancies between data quality and observed price volatility make it difficult to compare achieved performances to other studies. Very dense \ac{LOB}s and large price fluctuations in the bitcoin data set suggest, that large parts of the observed slippage stem from price fluctuations, which are harder to predict than slippage that originates from eating into the respective \ac{LOB}s.\\

An existing reinforcement learning approach, deriving valuable strategies from a discretized state space in a brute force manner, has been replicated and transferred to the data set at hand. While Nevmyvaka \etal \Cite{Nevmyvaka:2006} claimed improvements of $-27.16\%$ to $-35.50\%$ over more simple \ac{SL} strategies, if trained on private variables only, the replicated algorithm could only achieve an improvement of $-2.76\%$ on the bitcoin data set. If compared to the \ac{SL} strategy, that performed best on the training set rather than on the test set (see \Cref{chap:experiments:baseline}), this value increases to $-14.60\%$, which is still only half of what was claimed on traditional stock data. This observation gives reason to hope that the overhauled approaches perform better on other data sets.\\






\begin{enumerate}
\item An existing reinforcement learning approach, deriving valuable strategies from a discretized state space in a brute force manner, has been examined and was improved in detail. As an example, the proposed backward-sampling method neglected the agents own impact on the market situation. It was found, that the agents cost reducing capabilities may be further improved, if this self-induces impact is incorporated properly.

\item A novel reinforcement learning approach is presented, that applies growing batch reinforcement learning on the problem of optimal trade execution. The proposed forward-sampling method samples from a more realistic, continuous state space and outperforms the foremost algorithm.

\end{enumerate}

\section{Future Work}
bla